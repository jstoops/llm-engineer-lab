{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6ab9a2-28a2-445d-8512-a0dc8d1b54e9",
   "metadata": {},
   "source": [
    "Python to C++ Code Converter\n",
    "============================\n",
    "\n",
    "Converts Python code to C++ for performance:\n",
    "- Solution with a Frontier Model\n",
    "- Solution with an Open-Source model\n",
    "\n",
    "Download [results speadsheet here](../content/Python-to-CPP-Results.csv).\n",
    "\n",
    "Performance of C++ solution with identical results:\n",
    "- Claude 3.5 Sonnet is the winner, followed by GPT-4o, followed by CodeQwen\n",
    "- Note: Qwen has 7B parameters; its closed-source cousins have more than 1T\n",
    "- For everyday problems, Qwen is more than capable of converting Python to Optimized C++ code\n",
    "\n",
    "Simple coding problem result:\n",
    "- Both Frontier models and Qwen were able to write optimized C++ code for a simple program to estimate pi.\n",
    "- GPT-4o needed additional instructions to include libraries so C++ code would run\n",
    "- Qwen 1.5 still provided explanation text despite explicitly being told not to in the prompt\n",
    "- GPT-4o reduced time by 98.5% or 67x faster (186ms).\n",
    "- Claude 3.5 reduce time by 98.4% or 62x faster (204ms).\n",
    "- Qwen 1.5 reduced time by 98.5% or 67x faster (188ms).\n",
    "\n",
    "Harder coding problem result:\n",
    "- Both Frontier models were able to write optimized C++ code for a harder program to calculate the maximum subarray sum.\n",
    "- All models figured out the intent of the function and rewrote the code using [Kadane's Algorithm](https://en.wikipedia.org/wiki/Maximum_subarray_problem) to solve it which means it can be done in just 1 loop instead of a nested loop.\n",
    "- GPT-4o ran with a warning related to an overflow issue and produced the wrong result\n",
    "- Qwen 1.5 explanation code needed to be removed to ran and produced the wrong result due to changing the random number generation function \n",
    "- GPT-4o reduced time by 99.137% or 116x faster (415.427ms).\n",
    "- Claude 3.5 reduce time by 99.9984% or 64,579x faster (0.745ms).\n",
    "- Qwen 1.5 reduced time by 99.9987% or 77,069x faster (0.624ms).\n",
    "\n",
    "To do:\n",
    "- Add o1 (mini & preview), 03 mini, Gemini (2.0 Flash Exp & Thinking, Pro) to the Closed Source mix\n",
    "- Add CodeGemma, CodeLlama (Phind 34b v1, v2 & Python v1, 70b Python & Instruct), StarCoder 2 15b & WizardCoder 15b v1\n",
    "\n",
    "# Approach\n",
    "\n",
    "The approach used is to test models that rank well on the code related leaderboards given a prompt similer to below with the results compared using 2 coding problems: python code for a simple problem (loop that gradually approachs pii) and a harder problem ([calculate maximum subarray sum](https://en.wikipedia.org/wiki/Maximum_subarray_problem)).\n",
    "\n",
    "## Prompt\n",
    "> Please reimplement this Python code in C++ with the fastest possible implementation for `specify architecture or type of CPU and process`. Only respond with the C++ code. Do not explain your implementation. The only requirement is that the C++ code prints the same result and runs fast.\n",
    "\n",
    "## Leaderboards\n",
    "\n",
    "Best in Agentic Coding ([SWE Bench](https://arxiv.org/html/2410.06992v2)) on [Vellum LLM Leaderboard](https://www.vellum.ai/llm-leaderboard):\n",
    "1. 73.3% - Claude 3.7 Sonnet (R)\n",
    "2. 63.8% - Gemini 2.5 Pro\n",
    "3. 62.3% - Claude 3.7 Sonnet\n",
    "4. 61.0% - Open AI 03-mini\n",
    "5. 55.0% - GPT-4.1\n",
    "6. 51.8% - Gemini 2.0 Flash\n",
    "7. 49.2% - DeepSeek-R1\n",
    "8. 49.0% - Claude 3.5 Sonnet\n",
    "9. 48.9% - OpenAI o1\n",
    "10. 40.6% - Claude 3.5 Haiku\n",
    "\n",
    "...\n",
    "\n",
    "15. 18.8% - Qwen2.5-VL-32B\n",
    "16. 10.2% - Gemma 3 27b\n",
    "\n",
    "[Coding Leaderboard on Scale](https://scale.com/leaderboard/coding) (Deprecated as of March 2025):\n",
    "1. 1237 - o1-mini\n",
    "2. 1137 - o3-mini\n",
    "3. 1132 - GPT-4o (November 2024)\n",
    "4. 1123 - o1-preview\n",
    "5. 1111 - Gemini 2.0 Flash Experimental (December 2024)\n",
    "6. 1109 - Gemini 2.0 Pro (December 2024)\n",
    "7. 1108 - Gemini 2.0 Flash Thinking (January 2025)\n",
    "8. 1100 - DeepSeek R1\n",
    "9. 1083 - o1 (December 2024)\n",
    "10. 1079 - Claude 3.5 Sonnet (June 2024)\n",
    "11. 1045 - GPT-4o (August 2024)\n",
    "12. 1036 - GPT-4o (May 2024)\n",
    "13. 1034 - GPT-4 Turbo Preview\n",
    "13. 959 - Claude 3 Opus\n",
    "14. 1029 - Mistral Large 2\n",
    "15. 1022 - Llama 3.1 405B Instruct\n",
    "16. 1007 Gemini 1.5 Pro (August 27, 2024)\n",
    "17. 994 - Gemini 1.5 Pro (May 2024)\n",
    "18. 992 - GPT-4 (November 2024)\n",
    "19. 985 - Deepseek V3\n",
    "20. 984 - Llama 3.2 90B Vision Instruct\n",
    "22. 943 - Gemini 1.5 Flash\n",
    "23. 891 - Gemini 1.5 Pro (April 2024)\n",
    "24. 879 - Claude 3 Sonnet\n",
    "25. 871 - Llama 3 70B Instruct\n",
    "26. 811 - Mistral Large\n",
    "27. 685 - Gemini 1.0 Pro\n",
    "28. 598 - CodeLlama 34B Instruct\n",
    "\n",
    "[BigCode Models Leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard) to compare the performance of open-source models in code generation tasks.\n",
    "\n",
    "Include all, not just base models, that were not externally tested and have good python and cpp scores as well as good win rate sorted by best cpp score:\n",
    "1. 67.85 cpp / 87.20 python - CodeQwen1.5-7B-Chat\n",
    "2. 59.59 cpp / 71.95 python - Phind-CodeLlama-34B-v2\n",
    "3. 57.81 cpp / 65.85 python - Phind-CodeLlama-34B-v1\n",
    "4. 55.34 cpp / 70.22 python - Phind-CodeLlama-34B-Python-v1\n",
    "5. 49.69 cpp / 55.49 python - CodeLlama-70b-Python\n",
    "6. 49.69 cpp / 52.44 python - CodeLlama-70b\n",
    "7. 48.45 cpp / 75.60 python - CodeLlama-70b-Instruct\n",
    "8. 48.35 cpp / 50.79 python - CodeQwen1.5-7B\n",
    "9. 47.20 cpp / 70.73 python - WizardCoder-Python-34B-V1.0\n",
    "10. 42.86 cpp / 62.19 python - WizardCoder-Python-13B-V1.0\n",
    "11. 42.60 cpp / 52.74 python - CodeGemma-7B-it\n",
    "12. 41.53 cpp / 50.79 python - CodeLlama-34b-Instruct\n",
    "13. 41.44 cpp / 44.15 python - StarCoder2-15B\n",
    "14. 41.42 cpp / 45.11 python - CodeLlama-34b\n",
    "15. 40.34 cpp / 40.13 python - CodeGemma-7B\n",
    "16. 39.09 cpp / 53.29 python - CodeLlama-34b-Python\n",
    "17. 38.95 cpp / 58.12 python - WizardCoder-15B-V1.0\n",
    "18. 36.36 cpp / 50.60 python - CodeLlama-13b-Instruct\n",
    "19. 36.21 cpp / 42.89 python - CodeLlama-13b-Python\n",
    "20. 35.81 cpp / 35.07 python - CodeLlama-13b\n",
    "21. 33.63 cpp / 34.09 python - StarCoder2-7B\n",
    "22. 29.03 cpp / 45.65 python - CodeLlama-7b-Instruct\n",
    "\n",
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610bf56-a46e-4aff-8de1-ab49d62b1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import gradio as gr\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e9dad-59a8-4ba9-9da5-4815cb41eefa",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f672e1c-87e9-4865-b760-370fa605e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "\n",
    "if os.environ['OPENAI_API_KEY']:\n",
    "    print(f\"OpenAI API Key exists and begins {os.environ['OPENAI_API_KEY'][:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if os.environ['ANTHROPIC_API_KEY']:\n",
    "    print(f\"Anthropic API Key exists and begins {os.environ['ANTHROPIC_API_KEY'][:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if os.environ['HF_TOKEN']:\n",
    "    print(f\"HuggingFace Token Key exists and begins {os.environ['HF_TOKEN'][:3]}\")\n",
    "else:\n",
    "    print(\"HuggingFace Token not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa149ed-9298-4d69-8fe2-8f5de0f667da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "\n",
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "OPENAI_MODEL = \"gpt-4o\"\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20240620\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f060ecbb-2a28-4c52-9905-91e258d150d4",
   "metadata": {},
   "source": [
    "# Functions and Prompts\n",
    "\n",
    "## System prompt\n",
    "\n",
    "Replace with details on architecture code will run on such as AMD64 or specific CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896636f-923e-4a2c-9d6c-fac07828a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that reimplements Python code in high performance C++ for an AMD Ryzen 7 5800X 8-Core CPU with 3.80 GHz and Radeon RX 6800 XT GPU with 16GB GDDR6. \"\n",
    "system_message += \"Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. \"\n",
    "system_message += \"The C++ response needs to produce an identical output in the fastest possible time. Keep implementations of random number generators identical so that results match exactly.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d708aa4b-f6f5-4034-9e1d-4ff238c30313",
   "metadata": {},
   "source": [
    "## User prompt\n",
    "\n",
    "Note hints provided so GPT-4o included packages so code would run but still had an overflow error on the harder coding problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b3546-57aa-4c29-bc5d-f211970d04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(python):\n",
    "    user_prompt = \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
    "    user_prompt += \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
    "    user_prompt += \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
    "    user_prompt += python\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6190659-f54c-4951-bef4-4960f8e51cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e450d5e-6ca7-4ba8-bc0e-8769c44219e1",
   "metadata": {},
   "source": [
    "## Functions to generate code and output it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e1ba8c-5b05-4726-a9f3-8d8c6257350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a file called optimized.cpp\n",
    "\n",
    "def write_output(cpp):\n",
    "    code = cpp.replace(\"```cpp\",\"\").replace(\"```\",\"\")\n",
    "    with open(\"optimized.cpp\", \"w\") as f:\n",
    "        f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2fea8-74c6-4421-8f1e-0e76d5b201b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_gpt(python):    \n",
    "    stream = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for(python), stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        print(fragment, end='', flush=True)\n",
    "    write_output(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd84ad8-d55c-4fe0-9eeb-1895c95c4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_claude(python):\n",
    "    result = claude.messages.stream(\n",
    "        model=CLAUDE_MODEL,\n",
    "        max_tokens=2000,\n",
    "        system=system_message,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt_for(python)}],\n",
    "    )\n",
    "    reply = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            reply += text\n",
    "            print(text, end=\"\", flush=True)\n",
    "    write_output(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2140011-f197-4b9c-a2aa-e380f4ea40f9",
   "metadata": {},
   "source": [
    "# Coding Problems\n",
    "\n",
    "## Calculate Pii (easy) Python Code\n",
    "\n",
    "Loop that gradually approachs pii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cbb778-fa57-43de-b04b-ed523f396c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(100_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3d2de-213c-49d7-a0b9-c13ebc13f3e9",
   "metadata": {},
   "source": [
    "### Execute Python code to calculate pii\n",
    "\n",
    "Note down execution time of python script for baseline comparison.\n",
    "\n",
    "**Important: don't share notebook else can be used to execute code on your local machine. Always validate the code being executed isn't harmful before running.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe1cd4b-d2c5-4303-afed-2115a3fef200",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5e170-c959-40ab-9027-0099241c8900",
   "metadata": {},
   "source": [
    "### Optimize code using GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105db6f9-343c-491d-8e44-3a5328b81719",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_gpt(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d914bf3-8434-4e91-a54b-3eaa77a0cac3",
   "metadata": {},
   "source": [
    "### Run C++ code generated by GPT-4o\n",
    "\n",
    "Requires `clang++` installed on machine. Change to this to run on Linux or Mac:\n",
    "\n",
    "    !clang++ -O3 -std=c++17 -march=armv8.3-a -o optimized optimized.cpp\n",
    "    !./optimized\n",
    "\n",
    "Change -march to match machine architecture. Check valid options with to target specific system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b2443-7617-4431-9af3-824b3c024b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !clang++ -mcpu=help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194e40c-04ab-4940-9d64-b4ad37c5bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!clang++ -O3 -std=c++17 -march=x86-64 -o optimized.exe optimized.cpp\n",
    "!optimized.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ea424-d1da-4e4f-8e13-d9c5e7199369",
   "metadata": {},
   "source": [
    "### Optimize code using Claude 3.5 Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a11fe-e24d-4c65-8269-9802c5ef3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_claude(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a89b2-5641-40f0-a6a0-f175557d4cb8",
   "metadata": {},
   "source": [
    "### Run C++ code generated by Claude 3.5 Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a766f9-3d23-4bb4-a1d4-88ec44b61ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!clang++ -O3 -std=c++17 -march=x86-64 -o optimized.exe optimized.cpp\n",
    "!optimized.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827aafe5-f573-49a2-91bd-dd1c85175509",
   "metadata": {},
   "source": [
    "## Calculate Maximum Subarray Sum (hard)\n",
    "\n",
    "[Calculate maximum subarray sum problem](https://en.wikipedia.org/wiki/Maximum_subarray_problem): Given an array of a large number of random +ve and -ve numbers: if you were to take any subarray of consecutive numbers and add them up find the largest sum of any possible subarray.\n",
    "\n",
    "Uses custom function to create a large number of pseudo random numbers based on the [Linear Congruential Generator (LCG)](https://en.wikipedia.org/wiki/Linear_congruential_generator) algorithm instead of the coding langauge's built-in random number library.\n",
    "\n",
    "Note telling model to be careful to support large number sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b497b3-f569-420e-b92e-fb0f49957ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_hard = \"\"\"# Be careful to support large number sizes\n",
    "\n",
    "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
    "    value = seed\n",
    "    while True:\n",
    "        value = (a * value + c) % m\n",
    "        yield value\n",
    "        \n",
    "def max_subarray_sum(n, seed, min_val, max_val):\n",
    "    lcg_gen = lcg(seed)\n",
    "    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]\n",
    "    max_sum = float('-inf')\n",
    "    for i in range(n):\n",
    "        current_sum = 0\n",
    "        for j in range(i, n):\n",
    "            current_sum += random_numbers[j]\n",
    "            if current_sum > max_sum:\n",
    "                max_sum = current_sum\n",
    "    return max_sum\n",
    "\n",
    "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
    "    total_sum = 0\n",
    "    lcg_gen = lcg(initial_seed)\n",
    "    for _ in range(20):\n",
    "        seed = next(lcg_gen)\n",
    "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
    "    return total_sum\n",
    "\n",
    "# Parameters\n",
    "n = 10000         # Number of random numbers\n",
    "initial_seed = 42 # Initial seed for the LCG\n",
    "min_val = -10     # Minimum value of random numbers\n",
    "max_val = 10      # Maximum value of random numbers\n",
    "\n",
    "# Timing the function\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
    "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55683e10-bf67-498c-b3a9-ae61e233178c",
   "metadata": {},
   "source": [
    "### Execute Python code to calculate max subarray sum\n",
    "\n",
    "Note down execution time of python script for baseline comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab5e4bc-276c-4555-bd4c-12c699d5e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(python_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100958d5-f5f2-4692-b145-8c6ad3607316",
   "metadata": {},
   "source": [
    "### Optimize code using GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d24ed5-2c15-4f55-80e7-13a3952b3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_gpt(python_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57672cab-c070-48cb-b7d6-3bd44703aa3c",
   "metadata": {},
   "source": [
    "### Run C++ code generated by GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b3d073-88a2-40b2-831c-6f0c345c256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!clang++ -O3 -std=c++17 -march=x86-64 -o optimized.exe optimized.cpp\n",
    "!optimized.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b0337-36f2-4159-8566-590f4f354ec9",
   "metadata": {},
   "source": [
    "### Optimize code using Claude 3.5 Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9305446-1d0c-4b51-866a-b8c1e299bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_claude(python_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77158501-de69-48c8-95d0-49efb8ea73e1",
   "metadata": {},
   "source": [
    "### Run C++ code generated by Claude 3.5 Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c181036-8193-4fdd-aef3-fc513b218d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!clang++ -O3 -std=c++17 -march=x86-64 -o optimized.exe optimized.cpp\n",
    "!optimized.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1576874-6f29-4a85-940d-6eaaa32f94b9",
   "metadata": {},
   "source": [
    "# UI for Code Conversion\n",
    "\n",
    "## Functions to Stream Code Generate for UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be9f47d-5213-4700-b0e2-d444c7c738c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(python):    \n",
    "    stream = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for(python), stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        yield reply.replace('```cpp\\n','').replace('```','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669f56b-8314-4582-a167-78842caea131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_claude(python):\n",
    "    result = claude.messages.stream(\n",
    "        model=CLAUDE_MODEL,\n",
    "        max_tokens=2000,\n",
    "        system=system_message,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt_for(python)}],\n",
    "    )\n",
    "    reply = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            reply += text\n",
    "            yield reply.replace('```cpp\\n','').replace('```','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ae8f5-16c8-40a0-aa18-63b617df078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(python, model):\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(python)\n",
    "    elif model==\"Claude\":\n",
    "        result = stream_claude(python)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    for stream_so_far in result:\n",
    "        yield stream_so_far        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ddb38e-6b0a-4c37-baa4-ace0b7de887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        python = gr.Textbox(label=\"Python code:\", lines=10, value=python_hard)\n",
    "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        model = gr.Dropdown([\"GPT\", \"Claude\"], label=\"Select model\", value=\"GPT\")\n",
    "        convert = gr.Button(\"Convert code\")\n",
    "\n",
    "    convert.click(optimize, inputs=[python, model], outputs=[cpp])\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf2bff-a822-4009-a539-f003b1651383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_python(code):\n",
    "    try:\n",
    "        output = io.StringIO()\n",
    "        sys.stdout = output\n",
    "        exec(code)\n",
    "    finally:\n",
    "        sys.stdout = sys.__stdout__\n",
    "    return output.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3ab5d-fcfb-4d3f-8728-9cacbf833ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_cpp(code):\n",
    "    write_output(code)\n",
    "    compiler_cmd = [\"clang++\", \"-O3\", \"-std=c++17\", \"-march=x86-64\", \"-o\", \"optimized.exe\", \"optimized.cpp\"]\n",
    "    try:\n",
    "        compile_result = subprocess.run(compiler_cmd, check=True, text=True, capture_output=True)\n",
    "        run_cmd = [\"optimized.exe\"]\n",
    "        run_result = subprocess.run(run_cmd, check=True, text=True, capture_output=True)\n",
    "        return run_result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"An error occurred:\\n{e.stderr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba311ec-c16a-4fe0-946b-4b940704cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sample_code(sample_code):\n",
    "    if sample_code==\"pi\":\n",
    "        return pi\n",
    "    elif sample_code==\"python_hard\":\n",
    "        return python_hard\n",
    "    else:\n",
    "        return \"Type your Python program here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07176cbb-21ab-407d-8288-7bf2d6b3451f",
   "metadata": {},
   "source": [
    "## UI for Code Converter\n",
    "\n",
    "Note includes CodeQwen so run code below so functions are available for that and ensure model is deployed to AWS, Azure or GCP and running before selecting that option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2274f1-d03b-42c0-8dcc-4ce159b18442",
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    ".python {background-color: #306998;}\n",
    ".cpp {background-color: #050;}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1303932-160c-424b-97a8-d28c816721b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(css=css) as ui:\n",
    "    gr.Markdown(\"## Convert code from Python to C++\")\n",
    "    with gr.Row():\n",
    "        python = gr.Textbox(label=\"Python code:\", value=pi, lines=10)\n",
    "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        code = gr.Dropdown([\"pi\", \"python_hard\"], label=\"Select code\", value=\"pi\")\n",
    "        sample = gr.Button(\"Populate sample code\")\n",
    "        model = gr.Dropdown([\"GPT\", \"Claude\", \"CodeQwen\"], label=\"Select model\", value=\"GPT\")\n",
    "        convert = gr.Button(\"Convert code\")\n",
    "    with gr.Row():\n",
    "        python_run = gr.Button(\"Run Python\")\n",
    "        cpp_run = gr.Button(\"Run C++\")\n",
    "    with gr.Row():\n",
    "        python_out = gr.TextArea(label=\"Python result:\", elem_classes=[\"python\"])\n",
    "        cpp_out = gr.TextArea(label=\"C++ result:\", elem_classes=[\"cpp\"])\n",
    "\n",
    "    sample.click(select_sample_code, inputs=[code], outputs=[python])\n",
    "    convert.click(optimize, inputs=[python, model], outputs=[cpp])\n",
    "    python_run.click(execute_python, inputs=[python], outputs=[python_out])\n",
    "    cpp_run.click(execute_cpp, inputs=[cpp], outputs=[cpp_out])\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01724452-ff6d-42d2-9083-20227f68dded",
   "metadata": {},
   "source": [
    "# Open-Source Models Running in the Cloud\n",
    "\n",
    "Use [Inference Endpoints](https://endpoints.huggingface.co/) to have HuggingFace run models for you so you have an endpoint that you can use to call the model remotely from your code. **Remember to stop it!** Good alternative to Google Colab when you want to run the code locally but machine is not powerful enough.\n",
    "\n",
    "Steps:\n",
    "1. Select model to run from an endpoint. e.g. [CodeQwen1.5-7B-Chat](https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat)\n",
    "2. From the left toobar select Deploy->HF Inference Endpoint\n",
    "3. Select Cloud provider: Amazon Web Services, Miceosoft Azure or Google Cloud Platform\n",
    "4. Select runtime & region, e.g. for CodeQwen1.5-7B-Chat need at least a GPU box with 24GB of RAM such as `Nvidia L4`\n",
    "5. Create endpoint (requires credit card on file with HugglingFace)\n",
    "\n",
    "Note: can take 5-10 minutes to initalize endpoint when creating or unpausing.\n",
    "\n",
    "**Remember to stop it! Almost $1 per hour**\n",
    "\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c5b4e-ec51-4f21-b3f8-6aa94fede86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, InferenceClient\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df1857-5894-43f6-854e-a171d6cb0801",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Try running `# login(hf_token, add_to_git_credential=True)` and if issue picking up token run `login()` and paste token in textbox displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13347633-4606-4e38-9927-80c39e65c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)\n",
    "# login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0489940-5a2c-4040-93b8-5a7fac45d618",
   "metadata": {},
   "source": [
    "Replace Endpoint URL below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60a4df-6267-4ebd-8eed-dcb917af0a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_qwen = \"Qwen/CodeQwen1.5-7B-Chat\"\n",
    "# code_gemma = \"google/codegemma-7b-it\"\n",
    "CODE_QWEN_URL = \"https://w1zm0p9xt7jg0c0n.us-east4.gcp.endpoints.huggingface.cloud\"\n",
    "# CODE_GEMMA_URL = \"https://my-endpoint.region.cloud-provider.endpoints.huggingface.cloud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ce389-a903-4533-a2f1-cd9e2a6af8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(code_qwen)\n",
    "messages = messages_for(pi)\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4548e96-0b32-4793-bdd6-1b072c2f26ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b37cad-bfb6-4c2e-9a3b-c1113ad8ce36",
   "metadata": {},
   "source": [
    "## Optimize code using QwenCode 1.5 Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a126b-09e7-4966-bc97-0ef5c2cc7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(CODE_QWEN_URL, token=hf_token)\n",
    "stream = client.text_generation(text, stream=True, details=True, max_new_tokens=3000)\n",
    "for r in stream:\n",
    "    print(r.token.text, end = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a81da6a-3669-436e-b8ea-e3bab87ee907",
   "metadata": {},
   "source": [
    "### Function to Stream Qwen code generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a52e5-ad85-42b7-a0f5-9afda5efe090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_code_qwen(python):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(code_qwen)\n",
    "    messages = messages_for(python)\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    client = InferenceClient(CODE_QWEN_URL, token=hf_token)\n",
    "    stream = client.text_generation(text, stream=True, details=True, max_new_tokens=3000)\n",
    "    result = \"\"\n",
    "    for r in stream:\n",
    "        result += r.token.text\n",
    "        yield result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608b671-d198-43b7-a2e6-e58de8eac60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_code_qwen(python_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e776c-c1b7-4070-97b9-d335d246b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_code_qwen(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01db4ba3-37cc-4aa4-9795-450c06014afd",
   "metadata": {},
   "source": [
    "## Update function UI calls to support Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82387d1-7651-4923-995b-fe18356fcaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(python, model):\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(python)\n",
    "    elif model==\"Claude\":\n",
    "        result = stream_claude(python)\n",
    "    elif model==\"CodeQwen\":\n",
    "        result = stream_code_qwen(python)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    for stream_so_far in result:\n",
    "        yield stream_so_far    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ad093-425b-488e-8c3f-67f729dd9c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
