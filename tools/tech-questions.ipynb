{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# Tech Question AI Assistant\n",
    "\n",
    "A tool that takes a technical question, and responds with an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/v1\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the system prompt and payloads;\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert on LLMs and writing python code. You are able to answer complex questions with\n",
    "detailed answers and explain what every line of code does. You can refactor the code when asked.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get answer, with streaming\n",
    "\n",
    "def llm_copilot(question, model):\n",
    "    if 'llama' in model.lower():\n",
    "        openai = OpenAI(base_url=OLLAMA_API, api_key='ollama')\n",
    "    else:\n",
    "        openai = OpenAI()\n",
    "        \n",
    "    stream = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "          ],\n",
    "        stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Multi-shot prompting is a technique used in the context of language models, including large language models (LLMs), to improve the performance and relevance of generated responses. It involves providing the model with multiple examples of input-output pairs (prompts and their corresponding desired responses) in a single input request. This approach serves to guide the model in understanding the desired format and content of the output.\n",
       "\n",
       "### Key Aspects of Multi-shot Prompting:\n",
       "\n",
       "1. **Contextual Learning**: By presenting the model with several examples, it can better grasp patterns, nuances, and the specific context of the task. This enables the model to produce responses that are more aligned with the examples provided.\n",
       "\n",
       "2. **Task Definition**: Multi-shot prompting helps define the task clearly by demonstrating what is expected. It can show the model how to handle specific formats, answer types, or themes.\n",
       "\n",
       "3. **Increased Accuracy**: Providing multiple examples often leads to better results because the model can infer how to respond more appropriately based on the variations in the inputs shown.\n",
       "\n",
       "4. **Flexibility**: This approach is versatile and can be used for a variety of tasks—such as question-answering, text classification, and creative writing—by simply changing the examples provided.\n",
       "\n",
       "### Example:\n",
       "If you're asking a language model to complete sentences, a multi-shot prompt might look like this:\n",
       "\n",
       "\n",
       "Complete the following sentences:\n",
       "1. The capital of France is _______.\n",
       "2. The capital of Japan is _______.\n",
       "3. The capital of Italy is _______.\n",
       "\n",
       "\n",
       "In this scenario, the model learns that it must fill in the blanks with the names of capital cities, thereby improving its ability to generate accurate responses when similar prompts are encountered."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Ask question\n",
    "question = \"\"\"\n",
    "Provide a brief explanation of what multi-shot prompting is\n",
    "\"\"\"\n",
    "\n",
    "print(llm_copilot(question, MODEL_GPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4026cd-8967-4961-b26b-e3997307c4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
