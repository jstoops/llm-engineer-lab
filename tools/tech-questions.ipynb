{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# Tech Question AI Assistant\n",
    "\n",
    "A tool that takes a technical question, and responds with an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/v1\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the system prompt and payloads;\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert on LLMs and writing python code. You are able to answer complex questions with\n",
    "detailed answers and explain what every line of code does. You can refactor the code when asked.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get answer, with streaming\n",
    "\n",
    "def llm_copilot(question, model):\n",
    "    if 'llama' in model.lower():\n",
    "        openai = OpenAI(base_url=OLLAMA_API, api_key='ollama')\n",
    "    else:\n",
    "        openai = OpenAI()\n",
    "        \n",
    "    stream = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "          ],\n",
    "        stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To migrate your code from using LangChain to LangGraph and eliminate the `LangChainDeprecationWarning`, you'll need to follow a few steps to adapt the way you create your conversational chain.\n",
       "\n",
       "### Understanding the Context\n",
       "- **LangChain** was initially designed to work with language models, data loading, and reasoning tasks, whereas **LangGraph** aims to provide similar functionalities but with potential improvements in structure and functionality.\n",
       "- The concept of a conversational chain involves utilizing a language model (LLM), a retriever to fetch relevant information, and memory to maintain the context of the conversation.\n",
       "\n",
       "### Migration Steps\n",
       "\n",
       "Here's how you can migrate the code you provided:\n",
       "\n",
       "1. Install LangGraph if you haven't already:\n",
       "   bash\n",
       "   pip install langgraph\n",
       "   \n",
       "2. Import the necessary modules or classes from LangGraph.\n",
       "3. Replace the `ConversationalRetrievalChain` with the appropriate functionality provided by LangGraph.\n",
       "\n",
       "### Example Code Migration\n",
       "\n",
       "Assuming you have a similar structure in LangGraph, here's how you might adjust your code:\n",
       "\n",
       "#### Original LangChain Code\n",
       "python\n",
       "from langchain.chains import ConversationalRetrievalChain\n",
       "\n",
       "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)\n",
       "\n",
       "\n",
       "#### Migrated LangGraph Code\n",
       "python\n",
       "from langgraph.chains import ConversationalChain\n",
       "\n",
       "# This assumes that LangGraph has a similar method to create a conversational chain.\n",
       "conversation_chain = ConversationalChain.from_components(llm=llm, retriever=retriever, memory=memory)\n",
       "\n",
       "\n",
       "### Explanation of Lines\n",
       "- **Importing the Module**: \n",
       "  python\n",
       "  from langgraph.chains import ConversationalChain\n",
       "  \n",
       "  This line imports the necessary `ConversationalChain` class from the `langgraph.chains` module.\n",
       "\n",
       "- **Creating the Conversational Chain**: \n",
       "  python\n",
       "  conversation_chain = ConversationalChain.from_components(llm=llm, retriever=retriever, memory=memory)\n",
       "  \n",
       "  This line creates an instance of the `ConversationalChain` class by using the `from_components` method (or whatever method LangGraph provides). Here, `llm`, `retriever`, and `memory` would be the same objects you were using with LangChain.\n",
       "\n",
       "### Notes\n",
       "- Make sure to check the LangGraph documentation for any specific changes or adjustments that might be required, as the API and structure can differ.\n",
       "- Confirm that `llm`, `retriever`, and `memory` from your existing code are still compatible and correctly set up as per LangGraph requirements. There might be subtle differences in how these components are initialized or their method signatures.\n",
       "- If you encounter any changes in terms of method names or parameters, refer to the LangGraph documentation for guidance.\n",
       "\n",
       "By following these guidelines, you should be able to migrate your code effectively and eliminate any deprecation warnings associated with LangChain."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Ask question\n",
    "question = \"\"\"\n",
    "How do I migrate the following code to use LangGraph instead of LangChain to eliminate the LangChainDeprecationWarning?\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)\n",
    "\"\"\"\n",
    "\n",
    "print(llm_copilot(question, MODEL_GPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4026cd-8967-4961-b26b-e3997307c4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
